---
layout: page
title: Datasets and Tasks in HypoBench
permalink: /datasets/
---

## What is Hypothesis Generation?

Hypothesis generation is the process of proposing natural language theories or explanations about observed phenomena. This is a crucial step in scientific discovery and everyday reasoning. For example:

- In science: Inferring the heliocentric model from observations of planets and moons
- In daily life: Proposing reasons why one didn't get admitted to college

## Real World Datasets

HypoBench includes the following real-world datasets that span various domains:

### Deception Detection
- **Task Description**: Identify whether hotel reviews are genuine or fake.
- **IND Dataset**: Hotel reviews containing 800 genuine and 800 fake hotel reviews.
- **OOD Dataset**: Hotel reviews collected from different source websites and cities.

### AI-Generated Content (AIGC) Detection
- **Task Description**: Detect whether a story is AI-generated or human-written.
- **Dataset**: 800 writing prompts with both human-written stories and AI-generated stories.
- **IND Dataset**: Stories generated by the corresponding model.
- **OOD Dataset**: Stories generated by the alternative model.

### Persuasive Argument Prediction
- **Task Description**: Predict which of two texts is more persuasive.
- **Dataset**: Pairs of short texts with persuasive signals, with ground-truth labels annotated by humans.
- **IND and OOD Datasets**: Created based on different original sources of texts.

### Mental Stress Detection
- **Task Description**: Predict whether a Reddit post indicates mental stress.
- **Dataset**: 3.5k Reddit post segments with stress labels annotated via crowd-sourcing.
- **IND and OOD Datasets**: Partitioned based on the subreddits of the posts.

## Synthetic Datasets

HypoBench includes carefully controlled synthetic datasets at different complexity levels:

### College Admission Dataset
- **Task**: Predict college admission decisions
- **Controlled Factors**:
  - Number of features
  - Compositionality (depth of feature interactions)
  - Noise in outcome
  - Number of distractors

The synthetic datasets enable direct evaluation of how well models can recover known ground-truth hypotheses at varying levels of difficulty.

For more detailed information about the datasets and our methodology, please refer to our [paper](https://arxiv.org/abs/paper-link).

